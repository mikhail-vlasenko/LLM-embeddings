{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff119427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10fe05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the embeddings of the sentences\n",
    "import csv\n",
    "with open('../result/embedding.csv', 'r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    embedding_dict = [row for row in csv_reader]\n",
    "    \n",
    "# how to index the data: emdedding_dict[sentence_id][language_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b14653df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([124])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embedding_dict, sent_id, prompt_lang, sent_lang):\n",
    "    \"\"\"helper method to make indexing the data easier\"\"\"\n",
    "    return embedding_dict[sent_id][f'{prompt_lang}-{sent_lang}']\n",
    "\n",
    "emb_dim=124 #CHANGE THIS!!! \n",
    "# TEMPORARY!!!!!!!!!!!!!!!! (because the embedding dict doesn't work yet)\n",
    "def get_embedding(embedding_dict, sent_id, prompt_lang, sent_lang):\n",
    "    return (torch.zeros((emb_dim)))\n",
    "\n",
    "get_embedding(None, 1,\"English\", \"Russian\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "793fc583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(embedding_dict, 0, \"English\", \"Russian\")\n",
    "# with real data, this should not be all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a34af003",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['English', 'Chinese_Simplified', 'Russian', 'Dutch', 'German']\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embedding_dict, eng_prompt=True):\n",
    "        \"\"\"\n",
    "        embedding_dict: the dict with all the embeddings\n",
    "        eng_prompt: if true, uses English prompting, otherwise self-prompting\n",
    "        \"\"\"\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.eng_prompt=eng_prompt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embedding_dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        returns a dataset example of size (2, emb_dim) which represents embedding of \n",
    "        the same sentence in two (randomly chosen) different languages\n",
    "        \"\"\"\n",
    "        \n",
    "        # choose two different languages (this samples without replacement)\n",
    "        lang_1, lang_2 = random.sample(languages, 2)\n",
    "        \n",
    "        # english prompting\n",
    "        if self.eng_prompt: \n",
    "            return torch.stack([\n",
    "                get_embedding(self.embedding_dict, idx, \"English\", lang_1),\n",
    "                get_embedding(self.embedding_dict, idx, \"English\", lang_2),\n",
    "            ])\n",
    "        \n",
    "        # self-prompting\n",
    "        else: \n",
    "            return torch.stack([\n",
    "                get_embedding(self.embedding_dict, idx, lang_1, lang_1),\n",
    "                get_embedding(self.embedding_dict, idx, lang_2, lang_2),\n",
    "            ])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f509df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train,val,test sizes (60%, 20%, 20%)\n",
    "train_size, val_size, test_size = int(len(embedding_dict)*0.6), int(len(embedding_dict)*0.2), int(len(embedding_dict)*0.2)\n",
    "eng_prompt = True\n",
    "\n",
    "# make the datasets\n",
    "train_set = EmbeddingsDataset(embedding_dict[:train_size], eng_prompt=eng_prompt)\n",
    "val_set   = EmbeddingsDataset(embedding_dict[train_size:train_size+val_size], eng_prompt=eng_prompt)\n",
    "test_set  = EmbeddingsDataset(embedding_dict[train_size+val_size:], eng_prompt=eng_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2a05258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train, val, and test loader\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2506aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the MLP and optimizer\n",
    "mlp = torch.nn.Linear(emb_dim, emb_dim).to(\"cpu\")\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "913fcf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 124])\n",
      "torch.Size([16, 124])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)    \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# pass it through the mlp to get the new embeddings\u001b[39;00m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m mlp(batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# training loop: \n",
    "device = \"cpu\"\n",
    "\n",
    "# how long do we train? Do we look at the val loss to decide? \n",
    "# TODO: should add a second loop here\n",
    "\n",
    "for batch in train_loader:\n",
    "    # batch.shape: (bs, emb_dim)\n",
    "    print(batch.shape)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # flatten batch to (2*bs, emb_dim)\n",
    "    # flattens to (s1a, s1b, s2c, s2d, ...) where s1 is sentence 1, and a,b,... are languages\n",
    "    batch = batch.flatten(0,1)    \n",
    "    print(batch.shape) \n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    # pass it through the mlp to get the new embeddings\n",
    "    outputs = mlp(batch)\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    # apply the contrastive loss to the outputs\n",
    "    # positive examples same sentence, different languages\n",
    "    # negative examples: different sentences\n",
    "    \n",
    "    # TODO: \n",
    "    # loss = .....\n",
    "    \n",
    "    # take a step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.zero_grad():\n",
    "        # validation \n",
    "        # metric = ...\n",
    "        pass\n",
    "    \n",
    "    break # remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671c578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
