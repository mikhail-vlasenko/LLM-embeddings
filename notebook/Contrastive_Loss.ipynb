{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ff119427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10fe05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the embeddings of the sentences\n",
    "import csv\n",
    "with open('../result/embedding.csv', 'r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    embedding_dict = [row for row in csv_reader]\n",
    "    \n",
    "# how to index the data: emdedding_dict[sentence_id][language_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ace8d0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([124])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embedding_dict, sent_id, prompt_lang, sent_lang):\n",
    "    \"\"\"helper method to make indexing the data easier\"\"\"\n",
    "    return embedding_dict[sent_id][f'{prompt_lang}-{sent_lang}']\n",
    "\n",
    "emb_dim=124 #CHANGE THIS!!! \n",
    "# TEMPORARY!!!!!!!!!!!!!!!! (because the embedding dict doesn't work yet)\n",
    "def get_embedding(embedding_dict, sent_id, prompt_lang, sent_lang):\n",
    "    return (torch.zeros((emb_dim)))\n",
    "\n",
    "get_embedding(None, 1,\"English\", \"Russian\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f1246525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(embedding_dict, 0, \"English\", \"Russian\")\n",
    "# with real data, this should not be all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "094c0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['English', 'Chinese_Simplified', 'Russian', 'Dutch', 'German']\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embedding_dict, eng_prompt=True):\n",
    "        \"\"\"\n",
    "        embedding_dict: the dict with all the embeddings\n",
    "        eng_prompt: if true, uses English prompting, otherwise self-prompting\n",
    "        \"\"\"\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.eng_prompt=eng_prompt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embedding_dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        returns a dataset example of size (5, emb_dim) which represents the embeddings of \n",
    "        the same sentence in each of the languages\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        \n",
    "        for l in languages:\n",
    "            # english prompting\n",
    "            if self.eng_prompt: \n",
    "                out.append(get_embedding(self.embedding_dict, idx, \"English\", l))\n",
    "                \n",
    "            # self-prompting\n",
    "            else: \n",
    "                out.append(get_embedding(self.embedding_dict, idx, l, l))\n",
    "\n",
    "        return torch.stack(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2023fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 124])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define train,val,test sizes (60%, 20%, 20%)\n",
    "train_size, val_size, test_size = int(len(embedding_dict)*0.6), int(len(embedding_dict)*0.2), int(len(embedding_dict)*0.2)\n",
    "eng_prompt = True\n",
    "\n",
    "# make the datasets\n",
    "train_set = EmbeddingsDataset(embedding_dict[:train_size], eng_prompt=eng_prompt)\n",
    "val_set   = EmbeddingsDataset(embedding_dict[train_size:train_size+val_size], eng_prompt=eng_prompt)\n",
    "test_set  = EmbeddingsDataset(embedding_dict[train_size+val_size:], eng_prompt=eng_prompt)\n",
    "\n",
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2adc755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n",
      "torch.Size([8, 2, 124])\n"
     ]
    }
   ],
   "source": [
    "# make the train, val, and test loader\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for full_batch in train_loader:\n",
    "#     print(batch[:,(1,3),:].shape)\n",
    "    \n",
    "    \n",
    "    pairs = list(itertools.combinations(list(range(5)), 2))\n",
    "    for language_pair in pairs: \n",
    "        batch = full_batch[:,language_pair,:]\n",
    "        print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cb36f744",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mcombinations(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)), \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlanguages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4443e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the MLP and optimizer\n",
    "mlp = torch.nn.Linear(emb_dim, emb_dim).to(\"cpu\")\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6e8a9b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_batch.shape: torch.Size([8, 5, 124])\n",
      "batch.shape: torch.Size([8, 2, 124])\n",
      "flattened_batch.shape: torch.Size([16, 124])\n",
      "outputs.shape: torch.Size([16, 124])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 42\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# apply the contrastive loss to the outputs\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# positive examples same sentence, different languages\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# negative examples: different sentences\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# take a step\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzero_grad():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# validation \u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# metric = ...\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# training loop: \n",
    "device = \"cpu\"\n",
    "\n",
    "# how long do we train? Do we look at the val loss to decide? \n",
    "# TODO: should add a second loop here\n",
    "\n",
    "for full_batch in train_loader:\n",
    "#     full_batch.shape: (bs, 5, emb_dim)\n",
    "    print(f'full_batch.shape: {full_batch.shape}')\n",
    "    \n",
    "    pairs = list(itertools.combinations(list(range(5)), 2))\n",
    "    for language_pair in pairs: \n",
    "        # choose only two of the languages so we can do contrastive loss\n",
    "        # batch.shape: (bs, 2, emb_dim)\n",
    "        batch = full_batch[:,language_pair,:]\n",
    "        print(f'batch.shape: {batch.shape}')\n",
    "        \n",
    "        \n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # flatten batch to (2*bs, emb_dim)\n",
    "        # flattens to (s1a, s1b, s2c, s2d, ...) where s1 is sentence 1, and a,b,... are languages\n",
    "        batch = batch.flatten(0,1)    \n",
    "        print(f'flattened_batch.shape: {batch.shape}') \n",
    "\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # pass it through the mlp to get the new embeddings\n",
    "        outputs = mlp(batch)\n",
    "        print(f'outputs.shape: {outputs.shape}')\n",
    "\n",
    "        # apply the contrastive loss to the outputs\n",
    "        # positive examples same sentence, different languages\n",
    "        # negative examples: different sentences\n",
    "\n",
    "        # TODO: \n",
    "        # loss = .....\n",
    "\n",
    "        # take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    with torch.zero_grad():\n",
    "        # validation \n",
    "        # metric = ...\n",
    "        pass\n",
    "    \n",
    "    break # remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa3816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
